# Restricted Boltzmann Machines

## Energy-Based Models (EBM)

Energy-based models associate a scalar *energy* $E(\boldsymbol{x})$ to each configuration of the variables $\boldsymbol{x}$ of interest.

Energy-based models define the probability distribution through an energy function:

$$
P(\boldsymbol{x}) = \frac{e^{-E(\boldsymbol{x})}}{Z_1}
$$

where the normalizing constant $Z_1$ is called the *partition function* and is given by:

$$
Z_1 = \sum_{\boldsymbol{x}} e^{-E(\boldsymbol{x})}
$$

The summation in the partition function is taken over the sample space (all the configurations) of $\boldsymbol{x}$.

The learning of energy-based models corresponds to modifying the energy function such that its shape reaches desirable properties such as minimum energy.

### EBM Learning

An EBM is learned by performing (stochastic) gradient descent on the empirical negative log-likelihood of the training data. Thus, the cost function $J(\boldsymbol{\theta})$ is defined as:

$$
J(\boldsymbol{\theta}) = - \sum_p \log (P(\boldsymbol{x}_p))
$$

Parameters $\boldsymbol{\theta}$ (i.e. biases and weights) are found by computing the gradient $\frac{\partial J(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}$ and performing gradient descent.

### EBM w/ Hidden Units

In many cases of interest, we do not observe $\boldsymbol{x}$ fully or we want to include non-observed variables such as variables of hidden units.

Let $\boldsymbol{x}$ be the observed part (usually the input) and $\boldsymbol{h}$ be the non-observed part (i.e. hidden variables). Then, the join probability of both observed and non-observed variables can be written as:

$$
P(\boldsymbol{x}, \boldsymbol{h}) = \frac{e^{-E(\boldsymbol{x}, \boldsymbol{h})}}{Z}
$$

where

$$
Z = \sum_{\boldsymbol{x}, \boldsymbol{h}} e^{-E(\boldsymbol{x}, \boldsymbol{h})}
$$

Then, the probability of data $P(\boldsymbol{x})$ is obtained by marginalizing $P(\boldsymbol{x}, \boldsymbol{h})$ over the hidden variables $\boldsymbol{h}$:

$$
P(\boldsymbol{x}) = \sum_{\boldsymbol{h}} P(\boldsymbol{x}, \boldsymbol{h}) = \frac{\sum_{\boldsymbol{h}} e^{-E(\boldsymbol{x}, \boldsymbol{h})}}{Z} \quad \text{(A)}
$$

For energy-based models with hidden units, the notion of *free energy* $F(\boldsymbol{x})$ is used such that:

$$
P(\boldsymbol{x}) = \frac{e^{-F(\boldsymbol{x})}}{Z} \quad \text{(B)}
$$

where $Z = \sum_{\boldsymbol{x}} e^{-F(\boldsymbol{x})}$.

Thus, by comparing (A) & (B):

$$
F(\boldsymbol{x}) = - \log \Bigg(\sum_{\boldsymbol{h}} e^{-E(\boldsymbol{x}, \boldsymbol{h})} \Bigg)
$$

#### Gradient Descent

The gradient is defined as (complete derivation in Lecture Notes):

$$
\frac{\partial J(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} = \frac{\partial F(\boldsymbol{x})}{\partial \boldsymbol{\theta}} - \sum_{\boldsymbol{\tilde{x}}} p(\boldsymbol{\tilde{x}}) \frac{\partial F(\boldsymbol{\tilde{x}})}{\partial \boldsymbol{\theta}}
$$

where the LHS of the minus sign is the positive phase and the RHS is the negative phase. The positive phase gradients are generated by the training data and increase the cost while the negative phase gradients decrease the cost. Note that the samples $\boldsymbol{\tilde{x}}$ in the negative phase are generated by the network (i.e. drawn from the model).

From above:

$$
J(\boldsymbol{\theta}) = F(\boldsymbol{x}) - \sum_{\boldsymbol{\tilde{x}}} p(\boldsymbol{\tilde{x}}) F(\boldsymbol{\tilde{x}})
$$

If $N$ samples are drawn and equally likely:

$$
J(\boldsymbol{\theta}) = F(\boldsymbol{x}) - \frac{1}{N} \sum_{\boldsymbol{\tilde{x}}} F(\boldsymbol{\tilde{x}})
$$

Usually, $N = P$:

$$
J(\boldsymbol{\theta}) = F(\text{positive samples}) - F(\text{negative samples})
$$

That is, the positive phase of the cost function corresponds to the free energy computed over the training data (positive samples) and the negative phase corresponds to the free energy computed over the samples generated by the model (negative samples). The cost is given by the difference of the two computed free energies.

## Restricted Boltzmann Machines

![Restricted Boltzmann Machine](img/Restricted%20Boltzmann%20Machine.png)

A RBM is an EBM, defined by the energy function:

$$
E(\boldsymbol{x}, \boldsymbol{h}) = - \boldsymbol{b}^T \boldsymbol{x} - \boldsymbol{c}^T \boldsymbol{h} - \boldsymbol{h}^T \boldsymbol{W}^T \boldsymbol{x}
$$

- $\boldsymbol{x} = (x_1, x_2, \cdots x_n)^T$ are the activations of visible units (i.e. inputs).
- $\boldsymbol{h} = (h_1, h_2, \cdots h_M)^T$ are the activations of hidden units.
- $\boldsymbol{W} = \{w_{ij}\}_{n \times M}$ is the weight matrix connecting the inputs to the hidden units.
- $\boldsymbol{b}$ is the bias vector connected to the input layer (top down).
- $\boldsymbol{c}$ is the bias vector connected to the hidden layer (bottom up).

The energy of the RBM can also be written as:

$$
E(\boldsymbol{x}, \boldsymbol{h}) = - \sum_i b_i x_i - \sum_j c_j h_j - \sum_i \sum_j h_j w_{ij} x_i
$$

Note that the weights $w_{ij}$ are bidirectional.

Synaptic input $\boldsymbol{s}$ to the hidden units:

$$
\boldsymbol{s} = \boldsymbol{W}^T \boldsymbol{x} + \boldsymbol{c}
$$

Synaptic input to the $j$th hidden unit:

$$
s_j = {\boldsymbol{w}_j}^T \boldsymbol{x} + c_j
$$

Synaptic input $\boldsymbol{u}$ to the visible (input) units from hidden units:

$$
\boldsymbol{u} = \boldsymbol{W} \boldsymbol{h} + \boldsymbol{b}
$$

Synaptic input to the $i$th input unit:

$$
u_i = {{\boldsymbol{w}'}_i}^T h + b_i
$$

### Conditional Probabilities

The conditional probability of hidden activations given the input is given by:

$$
P(\boldsymbol{h} | \boldsymbol{x}) = \frac{P(\boldsymbol{h}, \boldsymbol{x})}{P(\boldsymbol{x})} = \frac{P(\boldsymbol{h}, \boldsymbol{x})}{\sum_{\boldsymbol{h}} P(\boldsymbol{h}, \boldsymbol{x})}
$$

After substituting $P(\boldsymbol{x}, \boldsymbol{h}) = \frac{e^{-E(\boldsymbol{x}, \boldsymbol{h})}}{Z}$ (complete derivation in Lecture Notes):

$$
P(\boldsymbol{h} | \boldsymbol{x}) = \prod_j P(h_j | \boldsymbol{x})
$$

Similarly,

$$
P(\boldsymbol{x} | \boldsymbol{h}) = \prod_i P(x_i | \boldsymbol{h})
$$

That is, because of the specific structure of RBM, visible and hidden units are conditionally independent given one another, where the conditional probabilities are given by:

$$
\begin{gathered}
P(h_j | \boldsymbol{x}) = \frac{e^{-E(\boldsymbol{x}, h_j)}}{\sum_{h_j} e^{-E(\boldsymbol{x}, h_j)}} \\
P(x_i | \boldsymbol{h}) = \frac{e^{-E(x_i, \boldsymbol{h})}}{\sum_{x_i} e^{-E(x_i, \boldsymbol{h})}}
\end{gathered}
$$

and $E(\boldsymbol{x}, h_j) = - h_j (c_j + \boldsymbol{x}^T \boldsymbol{w}_j)$ and $E(x_i, \boldsymbol{h}) = - x_i (b_i + \boldsymbol{h}^T {\boldsymbol{w}'}_i)$.

### Sampling in RBM

Samples of $P(\boldsymbol{x})$ are obtained by running a *Markov chain* to convergence, using *Gibbs sampling* as the transition operator.

A *Markov chain* is a sequence of states, which satisfies the Markov property â€” future states are determined only by the current state.

*Gibbs sampling* of joint distribution of $N$ random variables $\boldsymbol{s} = (s_1, s_2, \cdots s_N)$ is done through $N$ sampling subsets of the form $s_i \sim P(s_i | s_{-i})$ where $s_{-i}$ contains $N - 1$ variables in $\boldsymbol{s}$ excluding the variable $s_i$.

Gibbs sampling forms a Markov chain and when executed long enough, converges to the samples from the true distribution.

In RBM, the set of random variables consists of the set of hidden units and visible units. However, since they are conditionally independent, one can perform block Gibbs sampling. In this setting, visible units are sampled given the fixed values of hidden units (i.e. from $P(\boldsymbol{x} | \boldsymbol{h})$) and similarly, hidden units are sampled simultaneously given the values of visible units (i.e. from $P(\boldsymbol{h} | \boldsymbol{x})$).

For $t$ Gibbs sampling steps starting from the training samples (i.e. $\hat{P}(\boldsymbol{x})$):

$$
\begin{gathered}
\boldsymbol{x}^{(1)} \sim \hat{P}(\boldsymbol{x}) \\
\boldsymbol{h}^{(1)} \sim P(\boldsymbol{h} | \boldsymbol{x}^{(1)}) \\
\boldsymbol{x}^{(2)} \sim P(\boldsymbol{x} | \boldsymbol{h}^{(1)}) \\
\boldsymbol{h}^{(2)} \sim P(\boldsymbol{h} | \boldsymbol{x}^{(2)}) \\
\boldsymbol{x}^{(3)} \sim P(\boldsymbol{x} | \boldsymbol{h}^{(2)}) \\
\vdots \\
\boldsymbol{x}^{(t + 1)} \sim P(\boldsymbol{x} | \boldsymbol{h}^{(t)})
\end{gathered}
$$

It makes sense to start with training samples $\hat{P}(\boldsymbol{x})$ because as the model becomes better at capturing the structure, model distribution $P$ and $\hat{P}$ become similar.

As $t \rightarrow \infty$, samples $(\boldsymbol{x}^{(t)}, \boldsymbol{h}^{(t)})$ are guaranteed to be samples from $P(\boldsymbol{x}, \boldsymbol{h})$.

In theory, each parameter update in the learning process requires running a chain for convergence (to generate negative samples). This is very computationally prohibitive. As such, several algorithms have been devised in order to efficiently sample from $P(\boldsymbol{x}, \boldsymbol{h})$ during learning.

### RBM w/ Binary Units

RBMs are mostly studied with binary units. That is, $x_i, h_i \in \{0, 1\}$.

For this case (complete derivation in Lecture Notes):

$$
P(h_j = 1 | \boldsymbol{x}) = \text{logistic}(c_j + \boldsymbol{x}^T \boldsymbol{w}_j)
$$

where $s_j = c_j + \boldsymbol{x}^T \boldsymbol{w}_j$.

Similarly,

$$
P(x_i = 1 | \boldsymbol{h}) = \text{logistic}(b_i + \boldsymbol{h}^T  {\boldsymbol{w}'}_i)
$$

where $u_i = b_i + \boldsymbol{h}^T {\boldsymbol{w}'}_i$.

Outputs are sampled by drawing bits from binomial distribution with the above probabilities:

$$
\begin{gathered}
\boldsymbol{h} = \text{Binomial}\big(p = \text{logistic}(\boldsymbol{W}^T \boldsymbol{x} + \boldsymbol{c})\big) \\
\boldsymbol{x} = \text{Binomial}\big(p = \text{logistic}(\boldsymbol{W} \boldsymbol{h} + \boldsymbol{b})\big)
\end{gathered}
$$

The $t$-th step in the Markov chain becomes:

$$
\begin{gathered}
\boldsymbol{h}^{(t + 1)} \sim \text{logistic}(\boldsymbol{W}^T \boldsymbol{x}^{(t)} + \boldsymbol{c}) \\
\boldsymbol{x}^{(t + 1)} \sim \text{logistic}(\boldsymbol{W} \boldsymbol{h}^{(t)} + \boldsymbol{b})
\end{gathered}
$$

Since the outputs of hidden and visible units are binary, the samples are drawn from binomial distributions with probabilities $p$ given by the corresponding activations:

$$
\begin{gathered}
\boldsymbol{h}^{(t + 1)} = \text{Binomial}\big(p = \text{logistic}(\boldsymbol{W}^T \boldsymbol{x}^{(t)} + \boldsymbol{c})\big) \\
\boldsymbol{x}^{(t + 1)} = \text{Binomial}\big(p = \text{logistic}(\boldsymbol{W} \boldsymbol{h}^{(t)} + \boldsymbol{b})\big)
\end{gathered}
$$

### Free Energy of RBM

Substituting energy $E(\boldsymbol{x}, \boldsymbol{h})$ for RBM into the equation for free energy $F(\boldsymbol{x})$:

$$
F(\boldsymbol{x}) = - \boldsymbol{b}^T \boldsymbol{x} - \sum_j \log \sum_{h_j} e^{h_j (c_j + \boldsymbol{x}^T \boldsymbol{w}_j)}
$$

Since $E(\boldsymbol{x}, h_j) = -h_j(c_j + \boldsymbol{x}^T \boldsymbol{w}_j)$, free energy is given by:

$$
F(\boldsymbol{x}) = - \boldsymbol{b}^T \boldsymbol{x} - \sum_j \log \sum_{h_j} e^{E(\boldsymbol{x}, h_j)}
$$

For RBM with binary units:

$$
F(\boldsymbol{x}) = - \boldsymbol{b}^T \boldsymbol{x} - \sum_j \log \big(1 + e^{(c_j + \boldsymbol{x}^T \boldsymbol{w}_j)} \big)
$$

Note that the cost function is computed as the difference of free energies of positive (training) samples and negative (model) samples:

$$
J(\boldsymbol{\theta}) = F(\boldsymbol{x}) - \frac{1}{N} \sum_{\boldsymbol{\tilde{x}}} F(\boldsymbol{\tilde{x}})
$$

## Contrastive Divergence (CD-k)

Contrastive Divergence uses two tricks to speed up the sampling process:

1. Since we eventually want $P(\boldsymbol{x}) = \hat{P}(\boldsymbol{x})$ i.e. to be close to the training process, we initialize the Markov chain with a training example.
2. CD does not wait for the chain to converge and samples are taken after $k$ steps of Gibbs sampling. In practice, $k = 1$ has been shown to work surprisingly well.

## Persistent CD

Persistent CD uses another approximation for sampling from $P(\boldsymbol{x}, \boldsymbol{h})$. It relies on a single Markov chain, which has a persistent chain (i.e. not restarting the chain for each observed sample). For each parameter update, we extract new samples by simply running the chain for $k$ steps. The state of the chain is then preserved for subsequent updates.

The general intuition is that if parameter updates are small enough compared to the mixing rate of the chain, the Markov chain should be able to 'catch up' to the changes of the model.

## Tracking Progress of Training

RBMs are particularly tricky to train because of the partition function $Z$, the log likelihood $\log P(\boldsymbol{x})$ cannot be estimated during training. Therefore, we need some useful indicators to determine hyperparameters.

**Inspection of Negative Samples:** Negative samples generated by the model can be visualized. As the training progresses, the negative samples should look like the training samples.

**Visual Inspection of Filters:** Filters (weights) learned by the model can be visualized (as gray level images, for example). They should pick up useful features of data.

**Proxies to Likelihood:**

1. Reconstruction Error (Cross-Entropy)
2. Pseudo-Likelihood (for Persistent CD)

### Reconstruction Error

For input $\boldsymbol{x}$,

Synaptic input at hidden units $\boldsymbol{s} = \boldsymbol{W}^T \boldsymbol{x} + \boldsymbol{c}$.

Hidden layer activations: $P(\boldsymbol{h} = 1 | \boldsymbol{x}) = f(\boldsymbol{s}) = \frac{1}{1 + e^{- \boldsymbol{s}}}$

Hidden layer output: $\boldsymbol{h} = \text{Binomial}(p = f(\boldsymbol{s}))$

Synaptic input to input layer $\boldsymbol{u} = \boldsymbol{W} \boldsymbol{h} + \boldsymbol{b}$

Input layer activations: $P(\boldsymbol{x} = 1 | \boldsymbol{h}) = f(\boldsymbol{u}) = \frac{1}{1 + e^{- \boldsymbol{u}}}$

Sampled input: $\boldsymbol{x} = \text{Binomial}(p = f(\boldsymbol{u}))$

Sampling is done $k$ times...

$$
\text{Cross-Entropy } = \sum_p \sum_{pi} x_{pi} f(u_{pi}) + (1 - x_{pi})(1 - f(u_{pi}))
$$

### Pseudo-Likelihood for RBM

Pseudo-likelihood is less expensive to compute as it assumes that all the elements (i.e. the dimensions of input) are independent.

PL is defined as:

$$
\text{PL}(\boldsymbol{x}) = \prod_i P(x_i | x_{-i})
$$

or

$$
\log \text{PL}(\boldsymbol{x}) = \prod_i \log P(x_i | x_{-i})
$$

Note that $x_{-i}$ denotes the set of bits in $\boldsymbol{x}$ except the bit $x_i$. Therefore, the PL is the sum of log probabilities of each bit $x_i$, given the condition of the state of all other bits.

However, for high-dimensional data, summing over the RHS would be rather expensive. Therefore, the following approximation is used:

$$
\log \text{PL}(\boldsymbol{x}) = \frac{1}{m} \sum_{i \sim \text{Uniform}[1,n]} n \log P(x_i | x_{-i})
$$

where $n$ is the number of visible units and the summation is taken over $m$ visible units uniformly chosen from $[1,n]$. Often, $m$ is chosen to be 1.

$$
\log \text{PL}(\boldsymbol{x}) \approx n \log P(x_i | x_{-i}) = n \log \frac{e^{-F(\boldsymbol{x})}}{\sum_{\boldsymbol{x}} e^{-F(\boldsymbol{x})}}
$$

Let $\overline{\boldsymbol{x}_i}$ be the $\boldsymbol{x}$ with $x_i$ being flipped. Then, for binary units:

$$
\log \text{PL}(\boldsymbol{x}) \approx n \log \big(\text{logistic}(F(\overline{\boldsymbol{x}_i}) - F(\boldsymbol{x})) \big)
$$
